# Visualization Guide

This guide explains how to interpret the training visualizations generated by OptimizationLoss.

## Table of Contents

1. [Overview](#overview)
2. [Global Constraints Plot](#global-constraints-plot)
3. [Local Constraints Plot](#local-constraints-plot)
4. [Loss Components Plot](#loss-components-plot)
5. [Lambda Evolution Plot](#lambda-evolution-plot)
6. [Reading the Plots Together](#reading-the-plots-together)

---

## Overview

After training completes, four visualization files are created in `results/constraints_X_Y/`:

| File | Purpose |
|------|---------|
| `global_constraints.png` | Shows global prediction counts vs limits |
| `local_constraints.png` | Shows tracked course predictions vs limits |
| `losses.png` | Shows loss component evolution |
| `lambda_evolution.png` | Shows constraint weight changes |

All plots share the same X-axis: **Epoch number** (only epochs where metrics were logged, every 3 epochs).

---

## Global Constraints Plot

### What It Shows

This plot displays the total number of predicted students in each class over training, compared to the constraint limits.

### Visual Elements

- **Solid lines**: Actual predicted counts (hard predictions)
  - ðŸ”´ Red: Dropout predictions
  - ðŸ”µ Blue: Enrolled predictions
  - ðŸŸ¢ Green: Graduate predictions

- **Dashed lines**: Constraint limits
  - Horizontal lines showing the maximum allowed predictions per class
  - Only shown for constrained classes (Dropout, Enrolled)
  - Graduate typically has no limit (âˆž)

### Example Interpretation

```
Dropout Predictions (Red solid): Starts at 180, decreases to 140
Dropout Constraint (Red dashed): Horizontal line at 142
Status: âœ“ SATISFIED (140 â‰¤ 142)

Enrolled Predictions (Blue solid): Starts at 100, decreases to 81
Enrolled Constraint (Blue dashed): Horizontal line at 85
Status: âœ“ SATISFIED (81 â‰¤ 85)

Graduate Predictions (Green solid): Starts at 162, increases to 221
Graduate Constraint: No line (unconstrained)
```

### What to Look For

#### âœ… Healthy Training

![](https://via.placeholder.com/15/2ecc71/000000?text=+) Predictions converge **below** constraint lines
![](https://via.placeholder.com/15/2ecc71/000000?text=+) Lines become stable after warmup period
![](https://via.placeholder.com/15/2ecc71/000000?text=+) Smooth transitions, not erratic jumps

```
Epoch 0-250:   Lines fluctuate (warmup, no constraints)
Epoch 250-350: Lines adjust toward constraints
Epoch 350+:    Lines stable below constraint limits
```

#### âš ï¸ Warning Signs

![](https://via.placeholder.com/15/e74c3c/000000?text=+) Predictions oscillate wildly
![](https://via.placeholder.com/15/e74c3c/000000?text=+) Never converge below constraints
![](https://via.placeholder.com/15/e74c3c/000000?text=+) All predictions collapse to one class

**Collapse example:**
```
Dropout:  0 predictions (bad!)
Enrolled: 0 predictions (bad!)
Graduate: 442 predictions (everything!)
```
â†’ Model is ignoring classification, just satisfying constraints

---

## Local Constraints Plot

### What It Shows

This plot focuses on a **single course** (configured by `TRACKED_COURSE_ID`) to show how local constraints are satisfied.

### Visual Elements

- **Solid lines**: Predicted student counts for the tracked course
  - ðŸ”´ Red: Dropout predictions for this course
  - ðŸ”µ Blue: Enrolled predictions for this course
  - ðŸŸ¢ Green: Graduate predictions for this course

- **Dashed lines**: Per-course constraint limits
  - Only for constrained classes

### Example Interpretation

```
Course 2: Total students = 35

Dropout Predictions (Red): 8 students
Dropout Constraint (Red dashed): 10 maximum
Status: âœ“ SATISFIED (8 â‰¤ 10)

Enrolled Predictions (Blue): 5 students
Enrolled Constraint (Blue dashed): 6 maximum
Status: âœ“ SATISFIED (5 â‰¤ 6)

Graduate Predictions (Green): 22 students
```

### What to Look For

#### âœ… Good Behavior

- Predictions respect course-specific limits
- Smaller scale than global plot (fewer students)
- Similar convergence pattern to global constraints

#### âš ï¸ Problems

- Course predictions violate limits while global satisfied
  - â†’ Lambda weights not balancing global vs local
- Erratic behavior specific to this course
  - â†’ Check if course has unusual data distribution

---

## Loss Components Plot

### What It Shows

Four subplots showing different loss components over training:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ L_target        â”‚ L_feat          â”‚
â”‚ (Global)        â”‚ (Local)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ L_pred          â”‚ L_total         â”‚
â”‚ (Cross-Entropy) â”‚ (Combined)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. L_target (Global Constraint Loss)

**Top Left - Red Line**

- Measures global constraint violations
- Formula: `E / (E + K)` where E = max(0, predicted - limit)
- Range: 0 (satisfied) to ~1 (severe violation)

**Green dashed line**: Threshold (1e-6, essentially zero)

**Interpretation:**
```
High (>0.1):  Constraints severely violated
Medium (0.01-0.1): Moderate violations
Low (<0.01): Approaching satisfaction
~0:    Constraints satisfied
```

**Typical pattern:**
```
Epoch 0-250:   High (warmup, constraints ignored)
Epoch 250-300: Decreasing (Î» increases, model adjusts)
Epoch 300+:    Near zero (constraints satisfied)
```

### 2. L_feat (Local Constraint Loss)

**Top Right - Blue Line**

- Measures local (per-course) constraint violations
- Same formula as global, averaged across courses
- Green threshold line at 1e-6

**What to look for:**
- Should decrease after warmup
- Usually smaller than L_target (fewer violations)
- Converges to ~0 when satisfied

### 3. L_pred (Cross-Entropy Loss)

**Bottom Left - Orange Line**

- Standard classification loss
- Measures prediction accuracy on **labeled training data**
- No threshold line (always positive)

**Typical behavior:**
```
Epoch 0-250:   Decreases (learning classification)
Epoch 250+:    Slight increase (constraints pressure model)
```

**âš ï¸ Warning:** If CE loss increases significantly (>2x) after warmup:
- Constraints may be too tight
- Model sacrificing accuracy for constraint satisfaction
- Consider relaxing constraints

### 4. L_total (Combined Loss)

**Bottom Right - Purple Line**

- Sum of all losses: `L_total = L_pred + Î»_global * L_target + Î»_local * L_feat`
- Shows overall optimization objective

**Typical pattern:**
```
Epoch 0-250:   Decreases (CE dominates, Î»=0)
Epoch 250:     Sudden jump (constraints activate)
Epoch 250+:    Gradual decrease (finding balance)
```

---

## Lambda Evolution Plot

### What It Shows

How constraint weights (Î») change during training.

### Visual Elements

- ðŸ”´ Red line: Î»_global (global constraint weight)
- ðŸ”µ Blue line: Î»_local (local constraint weight)
- Circles/squares: Individual data points

### Interpretation

**Flat at 0** (epochs 0-250):
```
Warmup period - constraints disabled
```

**Increasing staircase** (after epoch 250):
```
Each step up = constraint violation detected
Step size = LAMBDA_STEP (default 0.01)
```

**Plateau**:
```
Constraints satisfied - no more increases needed
```

### Example Patterns

#### âœ… Healthy Pattern
```
Î»_global: 0 â†’ 0.05 â†’ 0.10 â†’ [plateau at 0.12]
Î»_local:  0 â†’ 0.03 â†’ 0.05 â†’ [plateau at 0.07]
```
- Both increase moderately
- Plateau indicates convergence
- Final values reasonable (<1.0)

#### âš ï¸ Problem Pattern
```
Î»_global: 0 â†’ 0.5 â†’ 1.0 â†’ 2.0 â†’ 5.0 â†’ [keeps growing]
```
- Never plateaus
- Values become very large (>1.0)
- Indicates constraints cannot be satisfied
- â†’ Constraints too tight OR gradient flow issue

### Annotations

The plot may include annotations showing:
- **Max Î»_global**: Highest value reached
- Useful for comparing different experiments

---

## Reading the Plots Together

### Scenario 1: Successful Training

**Global Constraints:**
- Lines converge below constraint limits âœ“

**Local Constraints:**
- Course predictions stable within limits âœ“

**Losses:**
- L_target â†’ 0 âœ“
- L_feat â†’ 0 âœ“
- L_pred stable/slightly increases âœ“
- L_total decreases then plateaus âœ“

**Lambda:**
- Increases then plateaus at reasonable values (<0.5) âœ“

**Conclusion:** Model successfully balanced accuracy and constraints

---

### Scenario 2: Constraint Violation

**Global Constraints:**
- Dropout line stays above constraint limit âœ—

**Local Constraints:**
- Some courses still violating âœ—

**Losses:**
- L_target stays high (>0.01) âœ—
- L_feat stays moderate âœ—

**Lambda:**
- Keeps increasing without plateau âœ—
- Reaches very high values (>2.0) âœ—

**Diagnosis:** Constraints too tight for this dataset
**Solution:** Relax constraints (increase percentages)

---

### Scenario 3: Class Collapse

**Global Constraints:**
- Dropout: 0 predictions
- Enrolled: 0 predictions
- Graduate: 442 predictions (all students)

**Losses:**
- L_target = 0 âœ“ (constraints "satisfied")
- L_pred = 0.7 (high! poor classification)

**Lambda:**
- May have increased too fast

**Diagnosis:** Model gaming constraints by predicting everything as unconstrained class
**Solution:**
- Increase warmup period
- Reduce initial lambda values
- Add constraints on Graduate class

---

### Scenario 4: No Convergence (700+ epochs)

**All plots:**
- Predictions fluctuate
- Losses oscillate
- Lambdas keep growing
- No stable pattern

**Diagnosis:** Gradient flow issue (should be fixed in current version)
**Verification:** Check that `trainer.py` uses:
```python
model.eval()
test_logits = model(X_test_tensor)  # NOT torch.no_grad()
model.train()
```

---

## Plot File Locations

After running experiments, find plots at:

```
results/
â””â”€â”€ constraints_0.8_0.6/
    â”œâ”€â”€ global_constraints.png
    â”œâ”€â”€ local_constraints.png
    â”œâ”€â”€ losses.png
    â””â”€â”€ lambda_evolution.png
```

Open with any image viewer or include in reports/presentations.

---

## Advanced: Comparing Multiple Experiments

To compare different constraint configurations:

1. Run multiple experiments with different settings
2. Open plots side-by-side from different result folders
3. Compare:
   - How quickly constraints are satisfied
   - Final lambda values (lower = easier to satisfy)
   - CE loss impact (lower = better accuracy maintained)

**Example comparison:**
```
Tight constraints (0.5, 0.3):
- Î»_global peaks at 1.2
- CE loss increases to 0.8
- Converges in 450 epochs

Relaxed constraints (0.8, 0.6):
- Î»_global peaks at 0.15
- CE loss stays at 0.5
- Converges in 120 epochs
```
â†’ Relaxed constraints are more practical for this dataset

---

## Questions?

See [USAGE_GUIDE.md](USAGE_GUIDE.md) for general usage or check the code in `src/utils/visualization.py` for plot generation details.
